# -*- coding: utf-8 -*-
"""GAN Sample Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vvn40oMC6gaD0SvCyGk5lUo2aNuqiOD3
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import PIL
from PIL import Image
import keras
from keras.layers import Dense, Conv2D, BatchNormalization, Activation, Reshape, UpSampling2D, Conv2DTranspose
from keras.layers import AveragePooling2D, MaxPooling2D, Input, Flatten,Dropout
from keras.optimizers import Adam
from keras.regularizers import l2
from keras import backend as K
from keras.models import Model,load_model
from keras.callbacks import ModelCheckpoint,LearningRateScheduler,ReduceLROnPlateau
import cv2
import os, os.path
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import drive
drive.mount('/content/gdrive')

def Normalize(image,mean,std):
    for channel in range(3):
        image[:,:,channel]=(image[:,:,channel]-mean[channel])/std[channel]
    return image

imageDir = '/content/gdrive/My Drive/pokemon'

image_path_list = []
for file in os.listdir(imageDir):
    image_path_list.append(os.path.join(imageDir, file))

image = []
for imagePath in image_path_list:
    im = Image.open(imagePath).convert('RGB')
    im = im.resize((224, 224))
    im=np.array(im,dtype=np.float32)
    im=im/255
    im=Normalize(im,[0.485,0.456,0.406],[0.229,0.224,0.225])
    image.append(im)

#Generator starts from here
inp_generator = Input((1000, ))

x = Dense(7*7*512)(inp_generator)
print(x.shape)
x = Dropout(0.4)(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Reshape((7,7,512))(x)
x = UpSampling2D()(x)
x = Conv2DTranspose(256, kernel_size = [3,3], padding = 'same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = UpSampling2D()(x)
x = Conv2DTranspose(128, kernel_size = [3,3], padding = 'same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = UpSampling2D()(x)
x = Conv2DTranspose(64, kernel_size = [3,3], padding = 'same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
#x = MaxPooling2D(pool_size = [2,2], padding = 'same')(x)
x = UpSampling2D()(x)
x = Conv2DTranspose(16, kernel_size = [3,3], padding = 'same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = UpSampling2D()(x)
x = Conv2DTranspose(3, kernel_size = [3,3], padding = 'same')(x)
output_generator = Activation('sigmoid')(x)
generator_model = Model(inp_generator, output_generator)

print(generator_model.summary())

def resnet_block(inputs,num_filters,kernel_size,strides,activation='relu'):
    x=Conv2D(num_filters,kernel_size=kernel_size,strides=strides,padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(1e-3))(inputs)
    x=BatchNormalization()(x)
    if(activation):
        x=Activation('relu')(x)
    return x

inputs=Input((224,224,3))

# conv1
x=resnet_block(inputs,64,[7,7],2)

# conv2
x=MaxPooling2D([3,3],2,'same')(x)
for i in range(2):
    a=resnet_block(x,64,[3,3],1)
    b=resnet_block(a,64,[3,3],1,activation=None)
    x=keras.layers.add([x,b])
    x=Activation('relu')(x)

# conv3
a=resnet_block(x,128,[1,1],2)
b=resnet_block(a,128,[3,3],1,activation=None)
x=Conv2D(128,kernel_size=[1,1],strides=2,padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(1e-3))(x)
x=keras.layers.add([x,b])
x=Activation('relu')(x)

a=resnet_block(x,128,[3,3],1)
b=resnet_block(a,128,[3,3],1,activation=None)
x=keras.layers.add([x,b])
x=Activation('relu')(x)

# conv4
a=resnet_block(x,256,[1,1],2)
b=resnet_block(a,256,[3,3],1,activation=None)
x=Conv2D(256,kernel_size=[1,1],strides=2,padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(1e-3))(x)
x=keras.layers.add([x,b])
x=Activation('relu')(x)

a=resnet_block(x,256,[3,3],1)
b=resnet_block(a,256,[3,3],1,activation=None)
x=keras.layers.add([x,b])
x=Activation('relu')(x)

# conv5
a=resnet_block(x,512,[1,1],2)
b=resnet_block(a,512,[3,3],1,activation=None)
x=Conv2D(512,kernel_size=[1,1],strides=2,padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(1e-3))(x)
x=keras.layers.add([x,b])
x=Activation('relu')(x)

a=resnet_block(x,512,[3,3],1)
b=resnet_block(a,512,[3,3],1,activation=None)
x=keras.layers.add([x,b])
x=Activation('relu')(x)

x=AveragePooling2D(pool_size=7,data_format="channels_last")(x)
# out:1*1*512

y=Flatten()(x)
# out:512
y=Dense(1000,kernel_initializer='he_normal',kernel_regularizer=l2(1e-3))(y)
y=Dropout(0.5,noise_shape=None)(y)
outputs=Dense(1,kernel_initializer='he_normal',kernel_regularizer=l2(1e-3), activation = 'sigmoid')(y)

discriminator_model=Model(inputs=inputs,outputs=outputs)
print(discriminator_model.summary())

batch_size = 64
y_train = np.ones((batch_size,1))

discriminator_model.compile(loss='binary_crossentropy',
            optimizer='adam')

discriminator_model.trainable = False

v = discriminator_model(output_generator)

adv_model = Model(inp_generator, v)

adv_model.compile(loss='binary_crossentropy', optimizer = 'adam')

image = np.array(image)

epochs = 2000
for epoch in range(epochs):
    idx = np.random.randint(0, len(image), batch_size)
    imgs = image[idx]
    y_valid = np.ones((batch_size, ), dtype = int)
    y_fake = np.zeros((batch_size, ))
    noise = np.random.normal(0, 1, (batch_size, 1000))
    gen_imgs = generator_model.predict(noise)
    d_loss_real = discriminator_model.train_on_batch(imgs, y_valid)
    d_loss_fake = discriminator_model.train_on_batch(gen_imgs, y_fake)
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    noise = np.random.normal(0, 1, (batch_size, 1000))
    g_loss = adv_model.train_on_batch(noise, y_valid)
    print('Epoch ', str(epoch+1))
    print('Discriminator Loss : ',d_loss, end = ' || ')
    print('Generator Loss : ',g_loss)

